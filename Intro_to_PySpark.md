**Spark** is a platform for cluster computing allowing us to spread data and computations over clusters with multiple nodes (think of each node as a separate computer), both data processing and computation are performed in parallel over the nodes in the cluster. Parallel computation can make certain types of programming tasks much faster, but it comes with greater complexity

<details>
<summary> Getting to know PySpark</summary>

</details>

<details>
<summary>Manipulating data</summary>

</details>

<details>
<summary>Getting started with machine learning pipelines</summary>
</details>

<details>
<summary>Model tuning and selection</summary>
</details>
