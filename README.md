# Introduction to PySpark 

**Spark** is a platform for cluster computing allowing us to spread data and computations over clusters with multiple nodes (think of each node as a separate computer), both data processing and computation are performed in parallel over the nodes in the cluster. Parallel computation can make certain types of programming tasks much faster, but it comes with greater complexity

<details>
<summary> Getting to know PySpark</summary>

</details>

<details>
<summary>Manipulating data</summary>

</details>

<details>
<summary>Getting started with machine learning pipelines</summary>
</details>

<details>
<summary>Model tuning and selection</summary>
</details>

# Big Data Fundamentals with PySpark


<details>
<summary> Introduction to Big Data analysis with Spark</summary>

</details>

<details>
<summary>Programming in PySpark RDDâ€™s</summary>

</details>

<details>
<summary>PySpark SQL & DataFrames</summary>
</details>

<details>
<summary>Machine Learning with PySpark MLlib</summary>
</details>

# Cleaning Data with PySpark

# Feature Engineering with PySpark

# Machine Learning with PySpark

# Building Recommendation Engines with PySpark

# Other resource

- [PySpark vs Pandas: A Comprehensive Guide to Data Processing Tools](https://www.linkedin.com/pulse/pyspark-vs-pandas-comprehensive-guide-data-processing-deepak-lakhotia-hpfgc/)
